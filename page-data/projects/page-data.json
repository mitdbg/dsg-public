{"componentChunkName":"component---src-pages-projects-js","path":"/projects/","result":{"data":{"allMdx":{"nodes":[{"frontmatter":{"title":"DejaVid","link":"/dejavid","summary":"We propose a novel framework for Semantic Video Retrieval (SVR), where we aim to find videos within a corpus that are semantically similar to a given query video. Difficulties with this problem include identifying semantically relevant events in a video and matching events in videos despite events spanning different durations. One promising technique is Dynamic Time Warping (DTW), which is temporal deformation-invariant but typically only supports low-dimensional data. In this work, we propose a DTW-augmented neural network architecture that learns the semantic relevance of events and features in a video, enabling general-purpose SVR without hand-coded events or features.","status":"current","image":null}},{"frontmatter":{"title":"BRAD: Simplifying Cloud Data Processing with Learned Automated Data Meshes","link":"/brad","summary":"The last decade of database research has led to the prevalence of specialized systems for different workloads. Consequently, organizations often rely on a combination of specialized systems, organized in a Data Mesh. Data meshes present significant challenges for system administrators, including picking the right system for each workload, moving data between systems, maintaining consistency, and correctly configuring each system. Many non-expert end users (e.g., data analysts or app developers) either cannot solve their business problems, or suffer from sub-optimal performance or cost due to this complexity. We envision BRAD, a cloud system that automatically integrates and manages data and systems into an instance-optimized data mesh, allowing users to efficiently store and query data under a unified data model (i.e., relational tables) without knowledge of underlying system details. With machine learning, BRAD automatically deduces the strengths and weaknesses of each engine through a combination of offline training and online probing. Then, BRAD uses these insights to route queries to the most suitable (combination of) system(s) for efficient execution. Furthermore, BRAD automates configuration tuning, resource scaling, and data migration across component systems, and makes recommendations for more impactful decisions, such as adding or removing systems. As such, BRAD exemplifies a new class of systems that utilize machine learning and the cloud to make complex data processing more accessible to end users, raising numerous new problems in database systems, machine learning, and the cloud.\n","status":"current","image":null}},{"frontmatter":{"title":"FactorJoin Cardinality Estimation","link":"/factorjoin","summary":"Cardinality estimation is one of the most fundamental and challenging problems in query optimization. Neither classical nor learning-based methods yield satisfactory performance when estimating the cardinality of the join queries. They either rely on simplified assumptions leading to ineffective cardinality estimates or build large models to understand the data distributions, leading to long planning times and a lack of generalizability across queries. We propose a new framework FactorJoin for estimating join queries. FactorJoin combines the idea behind the classical join-histogram method to efficiently handle joins with the learning-based methods to accurately capture attribute correlation. Specifically, FactorJoin scans every table in a DB and builds single-table conditional distributions during an offline preparation phase. When a join query comes, FactorJoin translates it into a factor graph model over the learned distributions to effectively and efficiently estimate its cardinality. Unlike existing learning-based methods, FactorJoin does not need to de-normalize joins upfront or require executed query workloads to train the model. Since it only relies on single-table statistics, FactorJoin has small space overhead and is extremely easy to train and maintain. In our evaluation, FactorJoin can produce more effective estimates than the previous state-of-the-art learning-based methods, with 40x less estimation latency, 100x smaller model size, and 100x faster training speed at comparable or better accuracy. In addition, FactorJoin can estimate 10,000 sub-plan queries within one second to optimize the query plan, which is very close to the traditional cardinality estimators in commercial DBMS.","status":"current","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB0klEQVR42jWQ2W7aUABE+f8P6WMfUilqqyTQRGlCqprFxmD7esP3ejcYElbbnCpIndeRjs5MzxGCuSUIlgrhhWiahuP62MLHXAhG4wnm3EBlEaHy8ALBeDRm7TiUQlA7NrllMdF1kjimV9c1RbWhrGocN0S4LkmWI9yApUzxvICprmMYBmmaof3V0A0dpSJCITAtC9uxMU0Tx3HoSSlRsSLPM3R9irAsirJEG4+JlCJJU2QYopTieDwSxzHres3peED5Ho7rkGTltd/v9/TO5zPa+A8PT/fMhi8Mb2+Z9vsc8pyurpGTCdrdPbPHR6qy5DMykiwjiTmfk6YJfhiSZSm+79Nrm4YsT7GDBUUsyTyfQkqazYbL+5bZxODnzXcGP/pkWXEFLsOApTXHFgKZ5ghzQRwn2I5Lr+u6K11fjPAjn0/jC9BeoOk6bD/mbepg2oIoLvhyJ4mKHe1+zaHKOR9O7MoN9a7lcGrpNU3Dy9srX7/dcDfoU63WbD4atruWOC0x5i6G6bFaVZybC0OzYvvRst6X1O8lSMklz4jKI9nq8PnhiYm+YPD0yvNwRFHVV8PrtEjx8Os394NnXC/kf7qupVgnlNuci1IQBKw2R5Jqzz+T4EjsdEYv5QAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/57ea1302dc5563686e8a5ed5c9115c36/7179c/factorjoin.png","srcSet":"/static/57ea1302dc5563686e8a5ed5c9115c36/11ed4/factorjoin.png 125w,\n/static/57ea1302dc5563686e8a5ed5c9115c36/12eb6/factorjoin.png 250w,\n/static/57ea1302dc5563686e8a5ed5c9115c36/7179c/factorjoin.png 500w,\n/static/57ea1302dc5563686e8a5ed5c9115c36/240df/factorjoin.png 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/57ea1302dc5563686e8a5ed5c9115c36/66dfd/factorjoin.webp 125w,\n/static/57ea1302dc5563686e8a5ed5c9115c36/8802f/factorjoin.webp 250w,\n/static/57ea1302dc5563686e8a5ed5c9115c36/4882b/factorjoin.webp 500w,\n/static/57ea1302dc5563686e8a5ed5c9115c36/474be/factorjoin.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":197}}}}},{"frontmatter":{"title":"LucidScript","link":"/lucid-script","summary":"Data preparation has been seen as \"janitor work\" yet essential in data-to-insight pipelines. The increasing liberality of data is followed by an explosion in the diversity of data consumers. However, the required technical and domain expertise prevents many from performing extensive data preparation. Further, many seem to be stuck in a vicious cycle of writing one-off programs to process data. Recently, automating data preparation programs has been shown to improve many aspects of the pipeline, including data quality, research reproducibility, and user productivity. We propose a novel approach to automatically improve data preparation programs.","status":"current","image":null}},{"frontmatter":{"title":"Self-Organizing Data Containers","link":"/sdc","summary":"We propose a new self-organizing, self-optimizing, meta-data rich storage layer for the cloud, called a self-organizing data container (SDC), that enables order-of-magnitude performance improvements in data-intensive applications through instance-optimization, i.e., the adaptation of data representation to exploit both the distribution of the data and the workload operating on it. Unlike existing cloud storage systems like Delta Lake, Apache Iceberg, and Apache Hudi, SDCs capture both data and metadata, like access histories and distributional statistics, and are designed to be flexible enough to encompass a variety of modern high-performance representations for data analytics, including partitioning, replication, indexing, and materialization.","status":"current","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAC30lEQVR42iWQS28jVRBG+x+y5zewYIPEahazYDNEiA1CGkBo2AyMBAprEIsMGUWJIHZedseOHdv97tvd9/Z99Mt9IE5JpfqqSvWpdLxtrpivA5ZpzXwTsQwFkWrRtkPqhs0uJkgEtWmpbMeqcFSmo3Y92yjnMUgo6wblemLV4uV1y9A1+IlBG4dxDWdLyWdvfF4dr/j0h2uOjhd88vqOD351ODr6fc3Ld0te/nzPF+98Xry957s/d6imxyttz1MsMku/H+mGkcuw5st/Un66FXw9LXk7L/n8NOYqqwkrx/fTnKOLhG+ngtdXOa/OY36ZC6Tr8XLZsncd81hj6hZVdyzyEjNo1NCT1QWxVmRdTzdYrsIE0ynM2FFYiWoUauhwvSStHd6PJwEffTXhzYeIj7+54sWvD2yrCqUEd3HBZBOTJAlBXtAYxTSMUKYkLyuuV1v89YZIVLStJKkt3jRr+C1oeb/RHG8d75OOVZoR7pbM/AVOCYo0YDb3KUXCdRRTyARZCKRSKCWRUuJMRqIMXmV7WmCWWtJmzxPSeZgwdPWB7egUY6MPemgN090OpXMYR57juTY2fzbMdU/RjJztDCvVI+yeu0iwyFLiskCqAqEqtlWNVILJLmEjUrZC4Cc5eSUoK0FapYSywZPOkhvDUhr8SpMay6xUPJSCRBdEKudiu2OyWXEvMvyyZl6WPMqCuC6IZEYkcyIlKK3BM21JU2es4x0PccBDEuIaxdjX1EVAGKy4nt9ys/TJtMLYClmE6CqmEgG9qxj2jmHsqJsCz+oMnGE9u+Xy7BSZxdAZxjpHRWtuLv5mczthUNmBJ7Zicz/j/OQvVrMbRisZO3vgaFqFZ3TCKBN280vu/z1FBguGOmNUKSpccnHyB/70nK4I6HXB3kkaXaGKBKfLQ79vnw11U+HpRqJNdkhj82dtBcaVyHKHyB8pxAYlw8PsaWdtjmsKrBNom2Maefiu/t/wP9SExdbuRyG9AAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/811731006a6018a4ea5dc049b475c50f/272b5/sdc.png","srcSet":"/static/811731006a6018a4ea5dc049b475c50f/12979/sdc.png 125w,\n/static/811731006a6018a4ea5dc049b475c50f/e65a8/sdc.png 250w,\n/static/811731006a6018a4ea5dc049b475c50f/272b5/sdc.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/811731006a6018a4ea5dc049b475c50f/03c37/sdc.webp 125w,\n/static/811731006a6018a4ea5dc049b475c50f/6b6cd/sdc.webp 250w,\n/static/811731006a6018a4ea5dc049b475c50f/2a48b/sdc.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":323}}}}},{"frontmatter":{"title":"Serverless State Management Systems","link":"/ssms","summary":"Modern cloud developers face many distributed systems complexities when building disaggregated applications from cloud building blocks. We propose a new class of cloud services, called Serverless State Management Systems (SSMS), that abstracts away these complexities and transparently manages fault-tolerance, deployment, and scaling of a logical cloud application on physical cloud resources. An SSMS, analogous to a DBMS, provides three important abstractions for disaggregated applications: 1) a logical application model, similar to relational algebra, that describes application semantics but abstracts away the deployment details, 2) strong resilient programming primitives, similar to ACID transactions, that simplifies fault-tolerant programming in the cloud, and 3) smart, cost-based optimization schemes that automates scheduling, placement, and other details, similar to a query optimizer. SSMS is an overarching research direction that encapsulates several projects in cloud, distributed and concurrent systems.\n","status":"current","image":null}},{"frontmatter":{"title":"Stage query execution time prediction","link":"/stage","summary":"Query performance (e.g., execution time) prediction is a critical component of modern DBMSes. As a pioneering cloud data warehouse, Amazon Redshift relies on an accurate execution time prediction for many downstream tasks, ranging from high-level optimizations, such as automatically creating materialized views, to low-level tasks on the critical path of query execution, such as admission, scheduling, and execution resource control. Unfortunately, many existing execution time prediction techniques, including those used in Redshift, suffer from cold start issues, inaccurate estimation, and are not robust against workload/data changes. In this paper, we propose a novel hierarchical execution time predictor: the Stage predictor. The Stage predictor is designed to leverage the unique characteristics and challenges faced by Redshift. The Stage predictor consists of three model states: an execution time cache, a lightweight local model optimized for a specific DB instance with uncertainty measurement, and a complex global model that is transferable across all instances in Redshift. We design a systematic approach to use these models that best leverages optimality (cache), instance-optimization (local model), and transferable knowledge about Redshift (global model). Experimentally, we show that the Stage predictor makes more accurate and robust predictions while maintaining a practical inference latency and memory overhead. Overall, the Stage predictor can improve the average query execution latency by 20% on these instances compared to the prior query performance predictor in Redshift.","status":"current","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAACFElEQVR42k2S2W7aUABE/f//UanqmrRqG0oTsggS1rIabGy4trnXNl6BGEICOVV46khHmqcjjTSaPSyjNz8TL5qkSUzHXNAY2jSGM+YqpdgWmNYcIVz8MCGMc5ZxymOxw1soLHvGZGISJykqiNDGrS8Y7U9sghpKulx3Z/y6bVOu9RhYEqkC7NkcfaQTxjFRmuL7AfpUcNmXlDou3+pzOtMA13XRojjEcWyCMGS93uCHEbbwsObuqUdJdpK6nmRsTtEnJlm+QgYRlusjFiGun+AvY3a7LZrRHmC3hti9Ca/AKk5x+jpioGO0usz6OvunPdtii90dMW60sbtD5NhktYxZb/eIhY+QEXG2QVPDHs3LM8xGHV8qxp0e+tUNs/s65m0VvXLHVJ8gpjNkq4NZrWFVaywaDZK5i+FElO8m/KxNsbwEbWRUKenv0I0Gbq2OMxjy5ElelOIgFYUnsWv3GA8tngPF3rVwuiaT+ynxm3CR0LQy+s4GN1yhhZ6Ho2YI0+RtcxosiU2LjeOxFg6pLchXGVmekttzEmNEv9qnWuoRCcnj0zNh/kiSF+TrAk25igf9gYq4YZMVPL8cUNLHGJtMxgbCdGgv/jLIdGKVEPghu2LL6+GFw+HI/zkej2hSvl1DIn3JVeWO9x8+clW55vefS87Ov/Pj5wVfz865KJWZC4flcnl6hB8E+L5CyjfkCc/z+Ac94Ylh7mWsHgAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/18f5ae06f823afac3e127f90073ebe3e/1e56f/stage.png","srcSet":"/static/18f5ae06f823afac3e127f90073ebe3e/c58e6/stage.png 125w,\n/static/18f5ae06f823afac3e127f90073ebe3e/74dd0/stage.png 250w,\n/static/18f5ae06f823afac3e127f90073ebe3e/1e56f/stage.png 500w,\n/static/18f5ae06f823afac3e127f90073ebe3e/cef58/stage.png 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/18f5ae06f823afac3e127f90073ebe3e/37d43/stage.webp 125w,\n/static/18f5ae06f823afac3e127f90073ebe3e/0e6f1/stage.webp 250w,\n/static/18f5ae06f823afac3e127f90073ebe3e/f9920/stage.webp 500w,\n/static/18f5ae06f823afac3e127f90073ebe3e/2a4ad/stage.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":216}}}}},{"frontmatter":{"title":"SEED: Domain-Specific Data Curation With Large Language Models","link":"/seed","summary":"We present SEED, an LLM-as-compiler approach that automatically generates domain-specific data curation solutions via Large Language Models (LLMs). Once the user describes a task, input data, and expected output, the SEED compiler produces a hybrid pipeline that combines LLM querying with more cost-effective alternatives, such as vector-based caching, LLM-generated code, and small models trained on LLM-annotated data. SEED features an optimizer that automatically selects from the four LLM-assisted modules and forms a hybrid execution pipeline that best fits the task at hand. In comparison to solutions that use the LLM on every data record, SEED achieves state-of-the-art or comparable few-shot performance, while significantly reducing the number of LLM calls.","status":"current","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAELmAABC5gGC1oevAAABUklEQVR42j3LSU7DMBQA0Nz/HCCVG0ARSGwKi4JUAgpqg9M22I7t+Hv4iYd0QmLB27/ifD5fLhcw5mtdd504HA4ppdPp9Pj4cH8/z3922+3NbCalPB6PAyIhpGnIOIbiPz8tnjeb7050y9dXRJzf3d7MZogIAHVdX19fMcY0gJCyIaSua0QslBCgtVaq3W05Z1POU0poLadU9z0opXtA5xwYZ0wOIYfIKeWUxnEsZNt6gL6HXhv0iDgc8qQYWywW7c/P5/v788ubR4wxWGsVY6MxXqkU45Rzka3TXDQ7SlvWiR4Rc0qK86+qiiFUZblcrrzz6D0AdJQOxqLWIYSUUjFq0J3gnQLZg7FjCNM08f2+Kst1Ve0JofsWlPLGSM5b8o3GeA2IGEIognMeDDo/OBw8Dog5RqvUrmm2hHx+fJSr1Wa9lkIwSiWlVitrtbMGnfsFJPR960ko2/QAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/7be1306421f2091a3ecf6303bd507394/037c8/seed.png","srcSet":"/static/7be1306421f2091a3ecf6303bd507394/a140c/seed.png 125w,\n/static/7be1306421f2091a3ecf6303bd507394/24826/seed.png 250w,\n/static/7be1306421f2091a3ecf6303bd507394/037c8/seed.png 500w,\n/static/7be1306421f2091a3ecf6303bd507394/0e732/seed.png 1000w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/7be1306421f2091a3ecf6303bd507394/388c3/seed.webp 125w,\n/static/7be1306421f2091a3ecf6303bd507394/d281d/seed.webp 250w,\n/static/7be1306421f2091a3ecf6303bd507394/70ccc/seed.webp 500w,\n/static/7be1306421f2091a3ecf6303bd507394/f798f/seed.webp 1000w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":172}}}}},{"frontmatter":{"title":"Treeline: An Update-In-Place Key-Value Store for Modern Storage","link":"/treeline","summary":"Many modern key-value stores, such as RocksDB, rely on log-structured merge trees (LSMs). Originally designed for spinning disks, LSMs optimize for write performance by only making sequential writes. But this optimization comes at the cost of reads: LSMs must rely on expensive compaction jobs and Bloom filters—all to maintain reasonable read performance. For NVMe SSDs, we argue that trading off read performance for write performance is no longer always needed. With enough parallelism, NVMe SSDs have comparable random and sequential access performance. This change makes update-in-place designs, which traditionally provide excellent read performance, a viable alternative to LSMs. In our paper, we close the gap between log-structured and update-in-place designs on modern SSDs with the help of new components that take advantage of data and workload patterns. Specifically, we explore three key ideas: (A) record caching for efficient point operations, (B) page grouping for high-performance range scans, and (C) insert forecasting to reduce the reorganization costs of accommodating new records. We evaluate these ideas by implementing them in a prototype update-in-place key-value store called TreeLine. On YCSB, we find that TreeLine outperforms RocksDB and LeanStore by 2.20× and 2.07× respectively on average across the point workloads, and by up to 10.95× and 7.52× overall.","status":"past","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABcRAAAXEQHKJvM/AAADfklEQVR42mPw69RVT57q9LB4QfjDnNl+H6P6zfMYGBgYQleFMjMgACMDAwMbAwODAAMDgxwDAwMfAwMDO1QcFczfHyqx9GRy9/ZLdd3rLxROWXgsygUkXl9fz8TAAMIMyJgFaigrlM+IJIcVsHErMoiLB3KLaXmJSjDIM3AwMDAwKxkL8su5i0jy2DCIsmuxq3DqM0jL23NJaARICYMsCQ3VYvNKUpD3S1KXYlAPlrKyrFD+591m9j+oy+5fUJf9f9dGwz++7Rb/Jy7rygXZUjw97Yxrs95//w7bv77tVv8DOmz/eHWY/rcoUToKku9a7aG39mL236lbo/8zyNmKSJoUKMQ6VujFOpbqJdlk6ORZ5WqkWOZoZAVn+WqDw7PWzdW6VDPJpdow1qlcP8G5yjjOrlwnySRf3hMk37bQWXjB4cS0zmUBGQxCZkJ8JhlKOnoZMjoaUZLGMrZ87ioeIvZKLsLOnCqcMiANuilCMvopUgb6WbLa6v6iVqpRogaGmdJ6JomqShiBphEmYmRdofbbu8X8v2+r1T+vZvP/Ae22fxzqtf4b5cg5gdRYl6gvCul3+O/fZvPHp9nif2CH3Z/Abtv/1mXqh2ARWF9vzwLCDHI2/IKWBUpONhW6Lo7VunZmueqOZmnKjoZZcs66UfyCIA3mZaqa7vUGLm6Vxo4utQb2LtXGNvbF2g6m+fIGyI77//8/OBlxIMeyX1ISb0Spn5S/f7wAAxeDJAMDAzdUDpY0WPX0XLl9fOq5oGkRBDih8rwgDm/nen+viRv8tMLaTRelTfb4EljnNDdzqveXkEbjWQmF5l6p9ZYRwbm6/nnNDvGN00uXZkxx/5M70+e/b7Vebt9SP5egdB2voh77gKh8owSw6avOp19bdjJ15vIzKctXn8+8ARJcfSHj/vLTqbNWnUtbs+p8xq5FJ+Knrr+Ue3nFscKmTTdz/m++nfd/2o7w8jUXs24uPJLYtulG3r7JG6NvgfSyzNwbu2rq9ojKaTsi2+bsi18NEpy7P37DjF3hVTN3R3fM2B01edL2sLLZ++OWTNgYHDNnf+yleQfiL3at8IubsTtmXfe6oJwFh5On183w3QYOG0ljBi4VFQZ2cXEGbkFBQX5Q2IhqMfBISjJwiYuLc4vrMXDD2eIgNo+IjAyfkKAgAz+XKIOEkBADHz8/gyA3N4M4AMGGGe0rbD0nAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/94d927582edb121b9b568797f6d1b3c6/c0db1/architecture.png","srcSet":"/static/94d927582edb121b9b568797f6d1b3c6/8849b/architecture.png 125w,\n/static/94d927582edb121b9b568797f6d1b3c6/35f49/architecture.png 250w,\n/static/94d927582edb121b9b568797f6d1b3c6/c0db1/architecture.png 500w","sizes":"(min-width: 500px) 500px, 100vw"},"sources":[{"srcSet":"/static/94d927582edb121b9b568797f6d1b3c6/2d7e5/architecture.webp 125w,\n/static/94d927582edb121b9b568797f6d1b3c6/e578d/architecture.webp 250w,\n/static/94d927582edb121b9b568797f6d1b3c6/73619/architecture.webp 500w","type":"image/webp","sizes":"(min-width: 500px) 500px, 100vw"}]},"width":500,"height":329}}}}}]}},"pageContext":{}},"staticQueryHashes":["3649515864","465186600"],"slicesMap":{}}