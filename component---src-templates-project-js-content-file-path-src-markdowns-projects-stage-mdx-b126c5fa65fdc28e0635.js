"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[858],{845:function(e,t,a){a.r(t),a.d(t,{Head:function(){return b},default:function(){return w}});var n=a(1151),r=a(7294);function i(e){const t=Object.assign({p:"p",a:"a",h2:"h2"},(0,n.ah)(),e.components);return r.createElement(r.Fragment,null,r.createElement(t.p,null,"Query performance (e.g., execution time) prediction is a critical component of modern DBMSes. As a pioneering cloud data warehouse, Amazon Redshift relies on an accurate execution time prediction for many downstream tasks, ranging from high-level optimizations, such as automatically creating materialized views, to low-level tasks on the critical path of query execution, such as admission, scheduling, and execution resource control. Unfortunately, many existing execution time prediction techniques, including those used in Redshift, suffer from cold start issues, inaccurate estimation, and are not robust against workload/data changes.\r\nIn this paper, we propose a novel hierarchical execution time predictor: the Stage predictor. The Stage predictor is designed to leverage the unique characteristics and challenges faced by Redshift. The Stage predictor consists of three model states: an execution time cache, a lightweight local model optimized for a specific DB instance with uncertainty measurement, and a complex global model that is transferable across all instances in Redshift. We design a systematic approach to use these models that best leverages optimality (cache), instance-optimization (local model), and transferable knowledge about Redshift (global model). Experimentally, we show that the Stage predictor makes more accurate and robust predictions while maintaining a practical inference latency and memory overhead. Overall, the Stage predictor can improve the average query execution latency by 20% on these instances compared to the prior query performance predictor in Redshift."),"\n",r.createElement(t.p,null,"For more details, please refer to our ",r.createElement(t.a,{href:"https://arxiv.org/pdf/2403.02286.pdf"},"SIGMOD 2024 paper"),"."),"\n",r.createElement(t.h2,null,"Participants"),"\n",r.createElement(t.p,null,"Ziniu Wu, Parimarjan Negi, Tim Kraska from DSG and many others from AWS"))}var o=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,n.ah)(),e.components);return t?r.createElement(t,e,r.createElement(i,e)):i(e)},c=(a(1883),a(8738)),l=a(8032),s=a(5624),m=a(2788),d=a(3544),u=a(9357),h=a(7341),p=a(637);const f=m.default.h2.withConfig({displayName:"project__ProjectTitle",componentId:"sc-49rh51-0"})([""," color:white;padding-left:1rem;"],h.n_),g=(0,m.default)(c.Z).withConfig({displayName:"project__StyledBg",componentId:"sc-49rh51-1"})(["&::before,&::after{filter:brightness(40%);}display:flex;align-items:center;"]),y=e=>{let{data:t,children:a}=e;const{mdx:{frontmatter:{image:i,title:o}}}=t,c=(0,s.to)((0,l.c)(i));return r.createElement(d.Z,null,r.createElement(g,Object.assign({Tag:"div"},c,{style:{height:"400px"},className:"align-middle",backgroundColor:"#ebeef2"}),r.createElement(f,null,o)),r.createElement(p.im,null,r.createElement(n.Zo,null,a)))},b=()=>r.createElement(u.Z,{title:"Project"});function w(e){return r.createElement(y,e,r.createElement(o,e))}}}]);
//# sourceMappingURL=component---src-templates-project-js-content-file-path-src-markdowns-projects-stage-mdx-b126c5fa65fdc28e0635.js.map