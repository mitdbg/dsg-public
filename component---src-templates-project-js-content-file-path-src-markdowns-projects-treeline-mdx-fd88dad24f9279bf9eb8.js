"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[117],{1069:function(e,n,t){t.r(n),t.d(n,{Head:function(){return w},default:function(){return y}});var a=t(1151),r=t(7294);function s(e){const n=Object.assign({h2:"h2",p:"p",span:"span",a:"a",ul:"ul",li:"li"},(0,a.ah)(),e.components);return r.createElement(r.Fragment,null,r.createElement(n.h2,null,"LSMs and Modern NVMe SSDs: Still the Best Combination?"),"\n",r.createElement(n.p,null,"Many modern key-value stores, such as RocksDB and LevelDB, rely on\nlog-structured merge trees (LSMs). Originally designed for spinning disks, LSMs\noptimize for write performance by only making sequential writes. But this\noptimization comes at the cost of reads: LSMs must rely on expensive compaction\njobs and Bloom filters—all to maintain reasonable read performance. For\ntraditional disks (e.g., HDDs and SATA SSDs), this write versus read performance\ntrade-off has been the preferred choice. Random writes on traditional disks are\nprohibitively expensive, and so any design that minimizes the amount of random\nwrites outshines the competition. But is this trade-off still the right one for\nmodern storage devices?"),"\n",r.createElement(n.p,null,"We observe that NVMe SSDs no longer suffer the same significant random write\ndrawback as traditional disks. With enough request parallelism, NVMe SSDs can\nachieve their peak sequential write throughput through random writes. This\nnaturally leads us to a research question: how should a persistent key-value\nstore’s design change for NVMe SSDs where random writes are comparable to\nsequential writes in performance?"),"\n",r.createElement(n.h2,null,"Revisit Update-in-Place?"),"\n",r.createElement(n.p,null,"We believe that update-in-place designs can be the answer for larger-than-memory\nworkloads that are (i) read-heavy, or (ii) skewed write-heavy. Update-in-place\ndesigns, such as a classical disk-based B+ tree, offer excellent read\nperformance because each record is stored in a single location on disk—requiring\nonly one I/O to read (when inner nodes are cached in memory). High read\nperformance is desirable because read-heavy workloads such as caching or\nanalytics are common in practice."),"\n",r.createElement(n.p,null,"However, classical disk-based B+ trees also suffer from their own challenges.\nFirst, updating a single record on a page requires reading and writing the\nentire page, which leads to write amplification. Second, scans can lead to\nrandom reads because logically consecutive leaf pages are not necessarily stored\nsequentially on disk; on NVMe SSDs, we observe that random reads still\nunderperform sequential reads. Third, inserts also cause write amplification\nbecause of the need to “make space” in the on-disk structure to hold the new\nrecords."),"\n",r.createElement(n.h2,null,"TreeLine to the Rescue!"),"\n",r.createElement("p",{align:"center",width:"100%"},r.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<span\n      class="gatsby-resp-image-wrapper"\n      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 833px; "\n    >\n      <a\n    class="gatsby-resp-image-link"\n    href="/static/94d927582edb121b9b568797f6d1b3c6/5205c/architecture.png"\n    style="display: block"\n    target="_blank"\n    rel="noopener"\n  >\n    <span\n    class="gatsby-resp-image-background-image"\n    style="padding-bottom: 65.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABcRAAAXEQHKJvM/AAADfklEQVR42mPw69RVT57q9LB4QfjDnNl+H6P6zfMYGBgYQleFMjMgACMDAwMbAwODAAMDgxwDAwMfAwMDO1QcFczfHyqx9GRy9/ZLdd3rLxROWXgsygUkXl9fz8TAAMIMyJgFaigrlM+IJIcVsHErMoiLB3KLaXmJSjDIM3AwMDAwKxkL8su5i0jy2DCIsmuxq3DqM0jL23NJaARICYMsCQ3VYvNKUpD3S1KXYlAPlrKyrFD+591m9j+oy+5fUJf9f9dGwz++7Rb/Jy7rygXZUjw97Yxrs95//w7bv77tVv8DOmz/eHWY/rcoUToKku9a7aG39mL236lbo/8zyNmKSJoUKMQ6VujFOpbqJdlk6ORZ5WqkWOZoZAVn+WqDw7PWzdW6VDPJpdow1qlcP8G5yjjOrlwnySRf3hMk37bQWXjB4cS0zmUBGQxCZkJ8JhlKOnoZMjoaUZLGMrZ87ioeIvZKLsLOnCqcMiANuilCMvopUgb6WbLa6v6iVqpRogaGmdJ6JomqShiBphEmYmRdofbbu8X8v2+r1T+vZvP/Ae22fxzqtf4b5cg5gdRYl6gvCul3+O/fZvPHp9nif2CH3Z/Abtv/1mXqh2ARWF9vzwLCDHI2/IKWBUpONhW6Lo7VunZmueqOZmnKjoZZcs66UfyCIA3mZaqa7vUGLm6Vxo4utQb2LtXGNvbF2g6m+fIGyI77//8/OBlxIMeyX1ISb0Spn5S/f7wAAxeDJAMDAzdUDpY0WPX0XLl9fOq5oGkRBDih8rwgDm/nen+viRv8tMLaTRelTfb4EljnNDdzqveXkEbjWQmF5l6p9ZYRwbm6/nnNDvGN00uXZkxx/5M70+e/b7Vebt9SP5egdB2voh77gKh8owSw6avOp19bdjJ15vIzKctXn8+8ARJcfSHj/vLTqbNWnUtbs+p8xq5FJ+Knrr+Ue3nFscKmTTdz/m++nfd/2o7w8jUXs24uPJLYtulG3r7JG6NvgfSyzNwbu2rq9ojKaTsi2+bsi18NEpy7P37DjF3hVTN3R3fM2B01edL2sLLZ++OWTNgYHDNnf+yleQfiL3at8IubsTtmXfe6oJwFh5On183w3QYOG0ljBi4VFQZ2cXEGbkFBQX5Q2IhqMfBISjJwiYuLc4vrMXDD2eIgNo+IjAyfkKAgAz+XKIOEkBADHz8/gyA3N4M4AMGGGe0rbD0nAAAAAElFTkSuQmCC\'); background-size: cover; display: block;"\n  ></span>\n  <img\n        class="gatsby-resp-image-image"\n        alt="A figure showing TreeLine&#39;s design."\n        title=""\n        src="/static/94d927582edb121b9b568797f6d1b3c6/5205c/architecture.png"\n        srcset="/static/94d927582edb121b9b568797f6d1b3c6/5a46d/architecture.png 300w,\n/static/94d927582edb121b9b568797f6d1b3c6/0a47e/architecture.png 600w,\n/static/94d927582edb121b9b568797f6d1b3c6/5205c/architecture.png 833w"\n        sizes="(max-width: 833px) 100vw, 833px"\n        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"\n        loading="lazy"\n        decoding="async"\n      />\n  </a>\n    </span>'}}),r.createElement("br"),r.createElement(n.p,null,r.createElement("strong",null,"Figure 1:")," TreeLine's design, highlighting our key ideas. See\nSections 3 and 4 in ",r.createElement("a",{href:"https://www.vldb.org/pvldb/vol16/p99-yu.pdf"},"our\npaper")," for more details.")),"\n",r.createElement(n.p,null,"In this project, we develop a new design for NVMe SSDs that has the read\nbenefits of a classical update-in-place design while also mitigating its\ntraditional write drawbacks. In other words, observing that the fast random I/O\nof NVMe SSDs would be favorable for an update-in-place design, we propose\ntechniques to make such a design competitive across the board. Our new design\nleverages three complementary techniques: (A) record caching to reduce\nread/write amplification in skewed workloads, (B) page grouping to translate\nscans into sequential reads, and (C) insert forecasting to reduce the I/O needed\nto “make space” for new records. We implement these techniques in TreeLine, a\nnew update-in-place key-value store designed for NVMe SSDs."),"\n",r.createElement(n.p,null,"TreeLine buffers all writes in its record cache (key idea A), allowing it to (i)\nkeep hot records in memory for as long as possible, and (ii) batch writes that\ngo to the same on-disk page to amortize the I/O costs for updating a page.\nInstead of laying out pages randomly on disk, TreeLine uses linear models to\ngroup pages storing adjacent key ranges so that they are stored contiguously on\ndisk (key idea B). Doing so lets TreeLine (i) make long physical reads\n(benefitting scans), while (ii) still allowing it to access data at page\ngranularity for point reads. Linear models help TreeLine realize these benefits\nwith a small in-memory index as it only needs to index the page group boundaries\n(instead of every page boundary). Finally, to reduce the cost of inserts in an\nupdate-in-place design, TreeLine exploits the repetitiveness in skewed insert\nworkloads to forecast the location and volume of inserts it expects to receive\n(key idea C). It uses the forecast to leave appropriate space in its on-disk\npages to reduce how frequently it needs to reorganize its on-disk pages to\naccommodate the new records. TreeLine tracks the inserts observed across\ndifferent parts of the key space and extrapolates these trends forward."),"\n",r.createElement(n.p,null,"We evaluate TreeLine on YCSB using synthetic and real-world datasets. We compare\nTreeLine against (i) RocksDB, a widely-used LSM key-value store, and (ii)\nLeanStore, a state- of-the-art update-in-place key-value store. Across our point\nYCSB workloads with 1024 byte records, TreeLine outperforms RocksDB and\nLeanStore by 2.20x and 2.07x respectively on average. Although TreeLine makes\nrandom reads from disk, we find that it still outperforms RocksDB and LeanStore\nby 2.50x and 2.80x respectively with 16 threads on uniform scan-heavy workloads."),"\n",r.createElement(n.h2,null,"Read the Paper"),"\n",r.createElement(n.p,null,r.createElement(n.a,{href:"https://www.vldb.org/pvldb/vol16/p99-yu.pdf"},"Geoffrey X. Yu*, Markos Markakis*, Andreas Kipf*, Per-Åke Larson, Umar Farooq\nMinhas, Tim Kraska. TreeLine: An Update-In-Place Key-Value Store for Modern\nStorage. PVLDB Volume 16 Issue 1, 2022.")),"\n",r.createElement(n.ul,null,"\n",r.createElement(n.li,null,"Denotes equal contribution."),"\n"),"\n",r.createElement(n.h2,null,"Project Participants"),"\n",r.createElement(n.p,null,"Andreas Kipf, Tim Kraska, Markos Markakis, Geoffrey Yu"))}var i=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,a.ah)(),e.components);return n?r.createElement(n,e,r.createElement(s,e)):s(e)},o=(t(1883),t(8738)),l=t(8032),c=t(5624),d=t(2788),p=t(3544),u=t(9357),m=t(7341),h=t(637);const g=d.default.h2.withConfig({displayName:"project__ProjectTitle",componentId:"sc-49rh51-0"})([""," color:white;padding-left:1rem;"],m.n_),f=(0,d.default)(o.Z).withConfig({displayName:"project__StyledBg",componentId:"sc-49rh51-1"})(["&::before,&::after{filter:brightness(40%);}display:flex;align-items:center;"]),b=e=>{let{data:n,children:t}=e;const{mdx:{frontmatter:{image:s,title:i}}}=n,o=(0,c.to)((0,l.c)(s));return r.createElement(p.Z,null,r.createElement(f,Object.assign({Tag:"div"},o,{style:{height:"400px"},className:"align-middle",backgroundColor:"#ebeef2"}),r.createElement(g,null,i)),r.createElement(h.im,null,r.createElement(a.Zo,null,t)))},w=()=>r.createElement(u.Z,{title:"Project"});function y(e){return r.createElement(b,e,r.createElement(i,e))}}}]);
//# sourceMappingURL=component---src-templates-project-js-content-file-path-src-markdowns-projects-treeline-mdx-fd88dad24f9279bf9eb8.js.map